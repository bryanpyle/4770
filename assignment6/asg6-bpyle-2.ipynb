{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README\n",
    "asg6-bpyle\n",
    "\n",
    "Assumptions:\n",
    "    For part 2, I assumed that any job with timestamp of 0 can be ignored\n",
    "    Also any job with eventType of 1 can be ignored\n",
    "    Also any job that has a start with no end time or vice versa can be ignored\n",
    "    \n",
    "    For part 4, I calculated the \"total CPU usage\" for a job by adding together (exec_time * avg_usage) for all\n",
    "    tasks of a given job. I then took the total time of the job and divided the sum/total_time. I did this because\n",
    "    you said the units were CPUCore-Seconds PER SECOND.\n",
    "    \n",
    "\n",
    "Contributorship:\n",
    "    I worked on this assignment alone, using only course materials. I posted a discussion \n",
    "    and consulted with other students about the results of the mapreduce calls (no one I talked\n",
    "    to had the same answers but I think they are wrong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cypress-kinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!klist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "Create the two directories named job_events and task_usage under \n",
    "your home directory on HDFS and then store the decompressed \n",
    "job_events and task_usage files into these respective directories (5 points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp -r /scratch3/lngo/gtrace/job_events/ /scratch3/bpyle/gtrace/; \n",
    "!cp -r /scratch3/lngo/gtrace/task_usage/ /scratch3/bpyle/gtrace/;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gunzip /scratch3/bpyle/gtrace/job_events/*.csv.gz;\n",
    "!gunzip /scratch3/bpyle/gtrace/task_usage/*.csv.gz;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir job_events/; hdfs dfs -put /scratch3/bpyle/gtrace/job_events/*.csv ./job_events/;\n",
    "!hdfs dfs -mkdir task_usage/; hdfs dfs -put /scratch3/bpyle/gtrace/task_usage/*.csv ./task_usage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 2\n",
    "Calculate the total run time from SUBMIT to either EVICT, FAIL, \n",
    "FINISH, KILL, or LOST for each job (job_events) and return only \n",
    "the top 20 longest run time jobs. The printout should use a tab-separated \n",
    "format that includes jobID, the total run time, and the job's final status \n",
    "(EVICT, FAIL, FINISH, KILL, or LOST). Run this job inside your notebook \n",
    "and print out the result of this job (5 points).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Job Events Table\n",
    "1. timestamp\n",
    "2. missing info\n",
    "3. jobID\n",
    "4. event type\n",
    "5. user name\n",
    "6. scheduling class\n",
    "7. job name\n",
    "8. logical job name\n",
    "\n",
    "SUBMIT(0)\n",
    "\n",
    "EVICT(2)\n",
    "FAIL(3)\n",
    "FINISH(4)\n",
    "KILL(5)\n",
    "LOST(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jobEventsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jobEventsMapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "MAXTIMESTAMP = pow(2, 63) - 1\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.split(\",\")\n",
    "    try:\n",
    "        ts = int(line[0])\n",
    "        jobID = int(line[2])\n",
    "        eventType = int(line[3])\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    if ts != 0 and ts != MAXTIMESTAMP and eventType != 1:\n",
    "        print('%d\\t%d\\t%d' % (jobID, ts, eventType))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jobEventsReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jobEventsReducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "lastID = None\n",
    "lastType = None\n",
    "startTime = None\n",
    "elapsedTime = -1\n",
    "\n",
    "#these are all counters for debugging\n",
    "total = 0\n",
    "newStartEvents = 0\n",
    "startEvents = 0\n",
    "multiStartEvents = 0\n",
    "newEndEvents = 0\n",
    "endEvents = 0\n",
    "endEventsFirst = 0\n",
    "successes = 0\n",
    "noStart = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.split(\"\\t\")\n",
    "    try:\n",
    "        jobID = int(line[0])\n",
    "        ts = int(line[1])\n",
    "        eventType = int(line[2])\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    total += 1\n",
    "    \n",
    "    if eventType == 0:\n",
    "        startEvents += 1\n",
    "    if eventType != 0:\n",
    "        endEvents += 1\n",
    "    \n",
    "    if jobID != lastID:\n",
    "        startTime = None\n",
    "        lastID = jobID\n",
    "        if eventType == 0: # if start event\n",
    "            newStartEvents += 1\n",
    "            startTime = ts\n",
    "        else: #first event is end DO NOT USE\n",
    "            endEventsFirst += 1\n",
    "            \n",
    "    else: #if on same ID\n",
    "        if eventType != 0: # end event\n",
    "            newEndEvents += 1\n",
    "            if startTime != None:\n",
    "                elapsedTime = ts - startTime\n",
    "                print(\"%d\\t%d\\t%d\\n\" % (elapsedTime, jobID, eventType))\n",
    "                successes += 1\n",
    "                startTime = None\n",
    "            else:\n",
    "                noStart += 1\n",
    "        else: #start event in same ID\n",
    "            multiStartEvents += 1\n",
    "            startTime = ts\n",
    "    \n",
    "# Uncomment for debugging\n",
    "# print(\"New Start Events: %d\\n\" % (newStartEvents))\n",
    "# print(\"Start Events: %d\\n\" %(startEvents))\n",
    "# print(\"Multi Start Events: %d\\n\" % (multiStartEvents))\n",
    "# print(\"No Start: %d\\n\" % (noStart))\n",
    "# print(\"New End Events: %d\\n\" % (newEndEvents))\n",
    "# print(\"End Events First: %d\\n\" % (endEventsFirst))\n",
    "# print(\"End Events: %d\\n\" % (endEvents))\n",
    "# print(\"Total: %d\\n\" %(total))\n",
    "# print(\"Successes: %d\\n\" %(successes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jobEventsSorter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jobEventsSorter.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import bisect\n",
    "\n",
    "def getEventString(eventInt):\n",
    "    if eventInt == 0:\n",
    "        return(\"SUBMIT\")\n",
    "    elif eventInt == 1:\n",
    "        return(\"SCHEDULE\")\n",
    "    elif eventInt == 2:\n",
    "        return(\"EVICT\")\n",
    "    elif eventInt == 3:\n",
    "        return(\"FAIL\")\n",
    "    elif eventInt == 4:\n",
    "        return(\"FINISH\")\n",
    "    elif eventInt == 5:\n",
    "        return(\"KILL\")\n",
    "    elif eventInt == 6:\n",
    "        return(\"LOST\")\n",
    "    \n",
    "#SUBMIT(0) SCHEDULE(1) EVICT(2) FAIL(3) FINISH(4) KILL(5) LOST(6)\n",
    "\n",
    "items = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "        \n",
    "    line = line.strip().split(\"\\t\")\n",
    "    \n",
    "    try: \n",
    "        time = int(line[0])\n",
    "        jobID = int(line[1])\n",
    "        eventType = int(line[2])\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    bisect.insort(items, (int(time), int(jobID), int(eventType)))\n",
    "    if len(items) > 20:\n",
    "        items.remove(items[0])\n",
    "\n",
    "print(\"Top 20 Jobs\\n[ID]\\t[Time]\\t[Type]\\n\")\n",
    "for item in reversed(items):\n",
    "    print(\"%s\\t%s\\t%s\\n\" % (item[1], item[0], getEventString(item[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat job_events/part-*-of-00500.csv \\\n",
    "    2>/dev/null \\\n",
    "    | python jobEventsMapper.py \\\n",
    "    | sort \\\n",
    "    | python jobEventsReducer.py \\\n",
    "    | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `job_events/output-top-20-times': No such file or directory\n",
      "17/10/17 20:15:24 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./jobEventsMapper.py, ./jobEventsReducer.py] [/usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.0.3-8.jar] /hadoop_java_io_tmpdir/streamjob5712733909097439707.jar tmpDir=null\n",
      "17/10/17 20:15:26 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/17 20:15:26 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/17 20:15:27 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14792 for bpyle on ha-hdfs:dsci\n",
      "17/10/17 20:15:27 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14792 for bpyle)\n",
      "17/10/17 20:15:27 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "17/10/17 20:15:27 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 8787857212dae53ffae3b3113abc894e6743b4ab]\n",
      "17/10/17 20:15:27 INFO mapred.FileInputFormat: Total input paths to process : 500\n",
      "17/10/17 20:15:28 INFO mapreduce.JobSubmitter: number of splits:500\n",
      "17/10/17 20:15:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1505269880969_0480\n",
      "17/10/17 20:15:28 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14792 for bpyle)\n",
      "17/10/17 20:15:29 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "17/10/17 20:15:30 INFO impl.YarnClientImpl: Submitted application application_1505269880969_0480\n",
      "17/10/17 20:15:30 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1505269880969_0480/\n",
      "17/10/17 20:15:30 INFO mapreduce.Job: Running job: job_1505269880969_0480\n",
      "17/10/17 20:15:36 INFO mapreduce.Job: Job job_1505269880969_0480 running in uber mode : false\n",
      "17/10/17 20:15:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/10/17 20:15:46 INFO mapreduce.Job:  map 2% reduce 0%\n",
      "17/10/17 20:15:47 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "17/10/17 20:15:48 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "17/10/17 20:15:49 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "17/10/17 20:15:50 INFO mapreduce.Job:  map 14% reduce 0%\n",
      "17/10/17 20:15:51 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "17/10/17 20:15:52 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "17/10/17 20:15:53 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "17/10/17 20:15:54 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "17/10/17 20:15:55 INFO mapreduce.Job:  map 34% reduce 0%\n",
      "17/10/17 20:15:56 INFO mapreduce.Job:  map 39% reduce 0%\n",
      "17/10/17 20:15:57 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "17/10/17 20:15:58 INFO mapreduce.Job:  map 46% reduce 14%\n",
      "17/10/17 20:15:59 INFO mapreduce.Job:  map 50% reduce 14%\n",
      "17/10/17 20:16:00 INFO mapreduce.Job:  map 52% reduce 14%\n",
      "17/10/17 20:16:01 INFO mapreduce.Job:  map 53% reduce 17%\n",
      "17/10/17 20:16:02 INFO mapreduce.Job:  map 56% reduce 17%\n",
      "17/10/17 20:16:03 INFO mapreduce.Job:  map 58% reduce 17%\n",
      "17/10/17 20:16:04 INFO mapreduce.Job:  map 63% reduce 19%\n",
      "17/10/17 20:16:05 INFO mapreduce.Job:  map 69% reduce 19%\n",
      "17/10/17 20:16:06 INFO mapreduce.Job:  map 72% reduce 19%\n",
      "17/10/17 20:16:07 INFO mapreduce.Job:  map 75% reduce 24%\n",
      "17/10/17 20:16:08 INFO mapreduce.Job:  map 82% reduce 24%\n",
      "17/10/17 20:16:09 INFO mapreduce.Job:  map 85% reduce 24%\n",
      "17/10/17 20:16:10 INFO mapreduce.Job:  map 89% reduce 28%\n",
      "17/10/17 20:16:11 INFO mapreduce.Job:  map 90% reduce 28%\n",
      "17/10/17 20:16:12 INFO mapreduce.Job:  map 92% reduce 28%\n",
      "17/10/17 20:16:13 INFO mapreduce.Job:  map 94% reduce 31%\n",
      "17/10/17 20:16:14 INFO mapreduce.Job:  map 95% reduce 31%\n",
      "17/10/17 20:16:15 INFO mapreduce.Job:  map 96% reduce 31%\n",
      "17/10/17 20:16:16 INFO mapreduce.Job:  map 96% reduce 32%\n",
      "17/10/17 20:16:17 INFO mapreduce.Job:  map 97% reduce 32%\n",
      "17/10/17 20:16:18 INFO mapreduce.Job:  map 99% reduce 32%\n",
      "17/10/17 20:16:19 INFO mapreduce.Job:  map 99% reduce 33%\n",
      "17/10/17 20:16:24 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "17/10/17 20:16:25 INFO mapreduce.Job:  map 100% reduce 41%\n",
      "17/10/17 20:16:28 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "17/10/17 20:16:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/10/17 20:16:30 INFO mapreduce.Job: Job job_1505269880969_0480 completed successfully\n",
      "17/10/17 20:16:30 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=38184773\n",
      "\t\tFILE: Number of bytes written=158064932\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=331131436\n",
      "\t\tHDFS: Number of bytes written=8636338\n",
      "\t\tHDFS: Number of read operations=1503\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=500\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=423\n",
      "\t\tRack-local map tasks=77\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12340521\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=118881\n",
      "\t\tTotal time spent by all map tasks (ms)=4113507\n",
      "\t\tTotal time spent by all reduce tasks (ms)=39627\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4113507\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=39627\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=53031332244\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=510871284\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2012242\n",
      "\t\tMap output records=1336181\n",
      "\t\tMap output bytes=35512405\n",
      "\t\tMap output materialized bytes=38187767\n",
      "\t\tInput split bytes=54500\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=669745\n",
      "\t\tReduce shuffle bytes=38187767\n",
      "\t\tReduce input records=1336181\n",
      "\t\tReduce output records=694434\n",
      "\t\tSpilled Records=2672362\n",
      "\t\tShuffled Maps =500\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=500\n",
      "\t\tGC time elapsed (ms)=109479\n",
      "\t\tCPU time spent (ms)=2691790\n",
      "\t\tPhysical memory (bytes) snapshot=1217832534016\n",
      "\t\tVirtual memory (bytes) snapshot=6655927246848\n",
      "\t\tTotal committed heap usage (bytes)=1323696652288\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=331076936\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8636338\n",
      "17/10/17 20:16:30 INFO streaming.StreamJob: Output directory: job_events/output-top-20-times\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R job_events/output-top-20-times\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input job_events/part-*-of-00500.csv \\\n",
    "    -output job_events/output-top-20-times \\\n",
    "    -file ./jobEventsMapper.py \\\n",
    "    -mapper jobEventsMapper.py \\\n",
    "    -file ./jobEventsReducer.py \\\n",
    "    -reducer jobEventsReducer.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Jobs\r\n",
      "[ID]\t[Time]\t[Type]\r\n",
      "\r\n",
      "6272373604\t2219071470839\tFAIL\r\n",
      "\r\n",
      "6262216777\t2179583847857\tKILL\r\n",
      "\r\n",
      "6254863670\t2075738599153\tKILL\r\n",
      "\r\n",
      "6254863987\t2075736980619\tKILL\r\n",
      "\r\n",
      "6256219395\t2057085980979\tKILL\r\n",
      "\r\n",
      "6273334224\t1997920913533\tKILL\r\n",
      "\r\n",
      "6273473735\t1915434479654\tKILL\r\n",
      "\r\n",
      "6283813709\t1884643427720\tKILL\r\n",
      "\r\n",
      "6275743443\t1881727884505\tKILL\r\n",
      "\r\n",
      "6276433166\t1875768873954\tKILL\r\n",
      "\r\n",
      "6276523719\t1874887733830\tKILL\r\n",
      "\r\n",
      "6276685724\t1873363338733\tKILL\r\n",
      "\r\n",
      "6283245463\t1843085463781\tFAIL\r\n",
      "\r\n",
      "6271227810\t1842057638079\tKILL\r\n",
      "\r\n",
      "6288829356\t1810226800987\tKILL\r\n",
      "\r\n",
      "6288842611\t1809984463546\tKILL\r\n",
      "\r\n",
      "6295425701\t1796937871697\tKILL\r\n",
      "\r\n",
      "6256516525\t1787831764245\tKILL\r\n",
      "\r\n",
      "6276799325\t1782978547093\tKILL\r\n",
      "\r\n",
      "6288553115\t1775916679684\tKILL\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat job_events/output-top-20-times/part-00000 | python jobEventsSorter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 3\n",
    "Count the number of computing tasks for \n",
    "each unique job (task_usage) and sort \n",
    "the results using total number of computing \n",
    "tasks. Run this job inside your notebook\n",
    "and print out the top 20 lines of the result (10 points). \n",
    "\n",
    "1. timestamp\n",
    "2. missing info\n",
    "3. job ID\n",
    "4. task index - within the job\n",
    "5. machine ID\n",
    "6. event type\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting numTasksMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile numTasksMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "tasks = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\",\")\n",
    "    try:\n",
    "        jobID = int(line[2])\n",
    "        index = int(line[3])\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if jobID not in tasks:\n",
    "        tasks[jobID] = index\n",
    "    else:\n",
    "        if index > task[jobID]:\n",
    "            tasks[jobID] = index\n",
    "\n",
    "for key, value in tasks.items():\n",
    "    print(\"%d\\t%d\\n\" % (key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting numTasksReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile numTasksReducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "lastID = None\n",
    "maxIndex = -1\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    try:\n",
    "        jobID = int(line[0])\n",
    "        index = int(line[1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    if jobID == lastID:\n",
    "        if index > maxIndex:\n",
    "            maxIndex = index\n",
    "    else:\n",
    "        if lastID:\n",
    "            print(\"%d\\t%d\\n\" %(maxIndex, lastID))\n",
    "        lastID = jobID\n",
    "        maxIndex = index\n",
    "if lastID == jobID:\n",
    "    print(\"%d\\t%d\\n\" %(maxIndex, lastID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sortTasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sortTasks.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import bisect\n",
    "\n",
    "\n",
    "items = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    \n",
    "    try:\n",
    "        index = int(line[0])\n",
    "        jobID = int(line[1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    bisect.insort(items, (index, jobID))\n",
    "    if len(items) > 20:\n",
    "        items.remove(items[0])\n",
    "        \n",
    "print(\"Top 20 Task Count\\n[ID]\\t[Count]\\n\")\n",
    "for item in reversed(items):\n",
    "    print(\"%s\\t%s\\n\" % (item[1], item[0] + 1)) #add one because max index is not the max count. Need to account\n",
    "                                               # for index 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat ./task_usage/part-00000-of-00500.csv \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 20 \\\n",
    "    | python numTasksMapper.py \\\n",
    "    | sort \\\n",
    "    | python numTasksReducer.py \\\n",
    "    | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/10/17 23:03:41 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/bpyle/task_usage/output-top-20-tasks' to trash at: hdfs://dsci/user/bpyle/.Trash/Current/user/bpyle/task_usage/output-top-20-tasks1508295821525\n",
      "17/10/17 23:03:43 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./numTasksMapper.py, ./numTasksReducer.py] [/usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.0.3-8.jar] /hadoop_java_io_tmpdir/streamjob6713185405074756128.jar tmpDir=null\n",
      "17/10/17 23:03:45 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/17 23:03:45 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/17 23:03:45 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14841 for bpyle on ha-hdfs:dsci\n",
      "17/10/17 23:03:46 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14841 for bpyle)\n",
      "17/10/17 23:03:46 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "17/10/17 23:03:46 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 8787857212dae53ffae3b3113abc894e6743b4ab]\n",
      "17/10/17 23:03:46 INFO mapred.FileInputFormat: Total input paths to process : 500\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.232:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.219:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.205:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.204:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.223:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.229:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.206:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.203:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.228:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.196:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.198:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.220:1019\n",
      "17/10/17 23:03:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.238:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.236:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.200:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.222:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.224:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.225:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.207:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.197:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.227:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.210:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.234:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.226:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.230:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.231:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.233:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.211:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.199:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.202:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.237:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.209:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.218:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.208:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.235:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.201:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.239:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.216:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.217:1019\n",
      "17/10/17 23:03:47 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.221:1019\n",
      "17/10/17 23:03:47 INFO mapreduce.JobSubmitter: number of splits:1550\n",
      "17/10/17 23:03:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1505269880969_0519\n",
      "17/10/17 23:03:47 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14841 for bpyle)\n",
      "17/10/17 23:03:48 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "17/10/17 23:03:49 INFO impl.YarnClientImpl: Submitted application application_1505269880969_0519\n",
      "17/10/17 23:03:49 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1505269880969_0519/\n",
      "17/10/17 23:03:49 INFO mapreduce.Job: Running job: job_1505269880969_0519\n",
      "17/10/17 23:03:55 INFO mapreduce.Job: Job job_1505269880969_0519 running in uber mode : false\n",
      "17/10/17 23:03:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/10/17 23:04:06 INFO mapreduce.Job:  map 1% reduce 0%\n",
      "17/10/17 23:04:07 INFO mapreduce.Job:  map 2% reduce 0%\n",
      "17/10/17 23:04:08 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "17/10/17 23:04:09 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "17/10/17 23:04:10 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "17/10/17 23:04:11 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "17/10/17 23:04:12 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "17/10/17 23:04:13 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "17/10/17 23:04:14 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "17/10/17 23:04:15 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "17/10/17 23:04:16 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "17/10/17 23:04:17 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "17/10/17 23:04:18 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "17/10/17 23:04:19 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "17/10/17 23:04:20 INFO mapreduce.Job:  map 35% reduce 11%\n",
      "17/10/17 23:04:21 INFO mapreduce.Job:  map 37% reduce 11%\n",
      "17/10/17 23:04:22 INFO mapreduce.Job:  map 39% reduce 11%\n",
      "17/10/17 23:04:23 INFO mapreduce.Job:  map 43% reduce 13%\n",
      "17/10/17 23:04:24 INFO mapreduce.Job:  map 45% reduce 13%\n",
      "17/10/17 23:04:25 INFO mapreduce.Job:  map 47% reduce 13%\n",
      "17/10/17 23:04:26 INFO mapreduce.Job:  map 50% reduce 16%\n",
      "17/10/17 23:04:27 INFO mapreduce.Job:  map 53% reduce 16%\n",
      "17/10/17 23:04:28 INFO mapreduce.Job:  map 57% reduce 16%\n",
      "17/10/17 23:04:29 INFO mapreduce.Job:  map 60% reduce 19%\n",
      "17/10/17 23:04:30 INFO mapreduce.Job:  map 63% reduce 19%\n",
      "17/10/17 23:04:31 INFO mapreduce.Job:  map 66% reduce 19%\n",
      "17/10/17 23:04:32 INFO mapreduce.Job:  map 70% reduce 22%\n",
      "17/10/17 23:04:33 INFO mapreduce.Job:  map 73% reduce 22%\n",
      "17/10/17 23:04:34 INFO mapreduce.Job:  map 76% reduce 22%\n",
      "17/10/17 23:04:35 INFO mapreduce.Job:  map 79% reduce 25%\n",
      "17/10/17 23:04:36 INFO mapreduce.Job:  map 81% reduce 25%\n",
      "17/10/17 23:04:37 INFO mapreduce.Job:  map 83% reduce 25%\n",
      "17/10/17 23:04:38 INFO mapreduce.Job:  map 84% reduce 28%\n",
      "17/10/17 23:04:39 INFO mapreduce.Job:  map 85% reduce 28%\n",
      "17/10/17 23:04:40 INFO mapreduce.Job:  map 86% reduce 28%\n",
      "17/10/17 23:04:42 INFO mapreduce.Job:  map 86% reduce 29%\n",
      "17/10/17 23:04:43 INFO mapreduce.Job:  map 87% reduce 29%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/10/17 23:04:54 INFO mapreduce.Job:  map 88% reduce 29%\n",
      "17/10/17 23:05:19 INFO mapreduce.Job:  map 89% reduce 29%\n",
      "17/10/17 23:05:22 INFO mapreduce.Job:  map 89% reduce 30%\n",
      "17/10/17 23:05:45 INFO mapreduce.Job:  map 90% reduce 30%\n",
      "17/10/17 23:06:10 INFO mapreduce.Job:  map 91% reduce 30%\n",
      "17/10/17 23:06:36 INFO mapreduce.Job:  map 92% reduce 30%\n",
      "17/10/17 23:06:37 INFO mapreduce.Job:  map 92% reduce 31%\n",
      "17/10/17 23:07:01 INFO mapreduce.Job:  map 93% reduce 31%\n",
      "17/10/17 23:07:20 INFO mapreduce.Job:  map 94% reduce 31%\n",
      "17/10/17 23:07:32 INFO mapreduce.Job:  map 95% reduce 31%\n",
      "17/10/17 23:07:34 INFO mapreduce.Job:  map 95% reduce 32%\n",
      "17/10/17 23:07:45 INFO mapreduce.Job:  map 96% reduce 32%\n",
      "17/10/17 23:07:59 INFO mapreduce.Job:  map 97% reduce 32%\n",
      "17/10/17 23:08:10 INFO mapreduce.Job:  map 98% reduce 32%\n",
      "17/10/17 23:08:11 INFO mapreduce.Job:  map 98% reduce 33%\n",
      "17/10/17 23:08:12 INFO mapreduce.Job:  map 99% reduce 33%\n",
      "17/10/17 23:08:15 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "17/10/17 23:08:20 INFO mapreduce.Job:  map 100% reduce 35%\n",
      "17/10/17 23:08:23 INFO mapreduce.Job:  map 100% reduce 49%\n",
      "17/10/17 23:08:26 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "17/10/17 23:08:39 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "17/10/17 23:08:42 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "17/10/17 23:08:45 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "17/10/17 23:08:46 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/10/17 23:08:47 INFO mapreduce.Job: Job job_1505269880969_0519 completed successfully\n",
      "17/10/17 23:08:47 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=135653415\n",
      "\t\tFILE: Number of bytes written=524205506\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=178524573791\n",
      "\t\tHDFS: Number of bytes written=10080549\n",
      "\t\tHDFS: Number of read operations=4653\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=1550\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1305\n",
      "\t\tRack-local map tasks=245\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36342273\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=821487\n",
      "\t\tTotal time spent by all map tasks (ms)=12114091\n",
      "\t\tTotal time spent by all reduce tasks (ms)=273829\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12114091\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=273829\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=156174861172\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3530203468\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1232799308\n",
      "\t\tMap output records=13932260\n",
      "\t\tMap output bytes=107788889\n",
      "\t\tMap output materialized bytes=135662709\n",
      "\t\tInput split bytes=168950\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=654677\n",
      "\t\tReduce shuffle bytes=135662709\n",
      "\t\tReduce input records=13932260\n",
      "\t\tReduce output records=1309352\n",
      "\t\tSpilled Records=27864520\n",
      "\t\tShuffled Maps =1550\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1550\n",
      "\t\tGC time elapsed (ms)=335027\n",
      "\t\tCPU time spent (ms)=9929840\n",
      "\t\tPhysical memory (bytes) snapshot=4006860955648\n",
      "\t\tVirtual memory (bytes) snapshot=20611483455488\n",
      "\t\tTotal committed heap usage (bytes)=4282371801088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=178524404841\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10080549\n",
      "17/10/17 23:08:47 INFO streaming.StreamJob: Output directory: task_usage/output-top-20-tasks\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R task_usage/output-top-20-tasks\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input task_usage/part-*-of-00500.csv \\\n",
    "    -output task_usage/output-top-20-tasks \\\n",
    "    -file ./numTasksMapper.py \\\n",
    "    -mapper numTasksMapper.py \\\n",
    "    -file ./numTasksReducer.py \\\n",
    "    -reducer numTasksReducer.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Task Count\r\n",
      "[ID]\t[Count]\r\n",
      "\r\n",
      "6324858588\t758720\r\n",
      "\r\n",
      "6221861800\t618576\r\n",
      "\r\n",
      "6330981257\t369529\r\n",
      "\r\n",
      "6484993276\t288265\r\n",
      "\r\n",
      "6484359944\t271407\r\n",
      "\r\n",
      "6336594489\t239168\r\n",
      "\r\n",
      "6337197528\t229551\r\n",
      "\r\n",
      "6331417954\t211529\r\n",
      "\r\n",
      "6483991792\t210658\r\n",
      "\r\n",
      "6405910961\t172023\r\n",
      "\r\n",
      "6484189122\t169764\r\n",
      "\r\n",
      "6337545460\t160469\r\n",
      "\r\n",
      "6483748490\t157323\r\n",
      "\r\n",
      "6337022710\t147364\r\n",
      "\r\n",
      "6258695096\t144285\r\n",
      "\r\n",
      "6419322512\t134101\r\n",
      "\r\n",
      "6467588751\t131944\r\n",
      "\r\n",
      "6280685099\t113012\r\n",
      "\r\n",
      "6405537326\t108241\r\n",
      "\r\n",
      "6468875195\t100813\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat task_usage/output-top-20-tasks/part-00000 | python sortTasks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "Calculate total CPU usage time (CPU-core seconds per second) \n",
    "for each job (task_usage) and sort the results using total CPU \n",
    "usage time from highest to lowest. The printout should use a \n",
    "tab-separated format that includes jobID and the total CPU usage time. \n",
    "Run this job inside your notebook and print out the top 20 lines of the result (10 points).  \n",
    "\n",
    "1. start time of the measurement period\n",
    "2. end time of the measurement period\n",
    "3. job ID\n",
    "4. task index\n",
    "5. machine ID\n",
    "6. mean CPU usage rate\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cpuUsageMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cpuUsageMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "tasks = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\",\")\n",
    "    try:\n",
    "        jobID = int(line[2])\n",
    "        time = int(line[1]) - int(line[0])\n",
    "        rate = float(line[5])\n",
    "        usage = float(time) * rate\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if jobID not in tasks:\n",
    "        tasks[jobID] = (usage, time)\n",
    "    else:\n",
    "        newtuple = (tasks[jobID][0] + usage, tasks[jobID][1])\n",
    "        tasks[jobID] = newtuple\n",
    "        \n",
    "for key, value in tasks.items():\n",
    "    print(\"%d\\t%f\\t%d\\n\" % (key, value[0], value[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cpuUsageReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cpuUsageReducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "lastID = None\n",
    "totalTime = 0.0\n",
    "totalUsage = 0.0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    try:\n",
    "        jobID = int(line[0])\n",
    "        usage = float(line[1])\n",
    "        time = float(line[2])\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if lastID == jobID:\n",
    "        totalTime += time\n",
    "        totalUsage += usage\n",
    "    else:\n",
    "        if lastID:\n",
    "            print(\"%d\\t%f\\n\" % (lastID, totalUsage/totalTime))\n",
    "        totalTime = time\n",
    "        totalUsage = usage\n",
    "        lastID = jobID\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cpuUsageSorter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cpuUsageSorter.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import bisect\n",
    "\n",
    "jobs = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    try:\n",
    "        jobID = int(line[0])\n",
    "        cpuUsage = float(line[1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    bisect.insort(jobs, (cpuUsage, jobID))\n",
    "    if len(jobs) > 20:\n",
    "        jobs.remove(jobs[0])\n",
    "    \n",
    "print(\"Top 20 CPU Usage\\n[ID]\\t[CPU_Usage]\")\n",
    "for job in reversed(jobs):\n",
    "    print(\"%d\\t%f\\n\" % (job[1], job[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat ./task_usage/part-00000-of-00500.csv \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 20 \\\n",
    "    | python cpuUsageMapper.py \\\n",
    "    | sort \\\n",
    "    | python cpuUsageReducer.py \\\n",
    "    | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/10/17 20:31:35 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/bpyle/task_usage/output-top-20-cpu' to trash at: hdfs://dsci/user/bpyle/.Trash/Current/user/bpyle/task_usage/output-top-20-cpu\n",
      "17/10/17 20:31:37 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./cpuUsageMapper.py, ./cpuUsageReducer.py] [/usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.0.3-8.jar] /hadoop_java_io_tmpdir/streamjob1341711588475691070.jar tmpDir=null\n",
      "17/10/17 20:31:39 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/17 20:31:39 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/17 20:31:40 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14794 for bpyle on ha-hdfs:dsci\n",
      "17/10/17 20:31:40 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14794 for bpyle)\n",
      "17/10/17 20:31:40 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "17/10/17 20:31:40 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 8787857212dae53ffae3b3113abc894e6743b4ab]\n",
      "17/10/17 20:31:40 INFO mapred.FileInputFormat: Total input paths to process : 500\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.219:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.232:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.204:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.205:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.223:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.229:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.203:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.206:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.228:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.196:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.198:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.238:1019\n",
      "17/10/17 20:31:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.220:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.200:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.236:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.222:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.225:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.224:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.207:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.227:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.197:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.210:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.234:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.226:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.230:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.231:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.233:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.211:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.199:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.202:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.237:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.209:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.208:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.218:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.235:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.201:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.239:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.216:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.217:1019\n",
      "17/10/17 20:31:41 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.221:1019\n",
      "17/10/17 20:31:41 INFO mapreduce.JobSubmitter: number of splits:1550\n",
      "17/10/17 20:31:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1505269880969_0482\n",
      "17/10/17 20:31:41 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14794 for bpyle)\n",
      "17/10/17 20:31:42 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "17/10/17 20:31:43 INFO impl.YarnClientImpl: Submitted application application_1505269880969_0482\n",
      "17/10/17 20:31:43 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1505269880969_0482/\n",
      "17/10/17 20:31:43 INFO mapreduce.Job: Running job: job_1505269880969_0482\n",
      "17/10/17 20:31:49 INFO mapreduce.Job: Job job_1505269880969_0482 running in uber mode : false\n",
      "17/10/17 20:31:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/10/17 20:32:02 INFO mapreduce.Job:  map 1% reduce 0%\n",
      "17/10/17 20:32:03 INFO mapreduce.Job:  map 2% reduce 0%\n",
      "17/10/17 20:32:04 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "17/10/17 20:32:05 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "17/10/17 20:32:06 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "17/10/17 20:32:07 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "17/10/17 20:32:08 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "17/10/17 20:32:09 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "17/10/17 20:32:10 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "17/10/17 20:32:11 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "17/10/17 20:32:12 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "17/10/17 20:32:13 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "17/10/17 20:32:14 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "17/10/17 20:32:15 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "17/10/17 20:32:16 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "17/10/17 20:32:17 INFO mapreduce.Job:  map 29% reduce 9%\n",
      "17/10/17 20:32:18 INFO mapreduce.Job:  map 31% reduce 9%\n",
      "17/10/17 20:32:19 INFO mapreduce.Job:  map 32% reduce 9%\n",
      "17/10/17 20:32:20 INFO mapreduce.Job:  map 34% reduce 10%\n",
      "17/10/17 20:32:21 INFO mapreduce.Job:  map 36% reduce 10%\n",
      "17/10/17 20:32:22 INFO mapreduce.Job:  map 38% reduce 10%\n",
      "17/10/17 20:32:23 INFO mapreduce.Job:  map 40% reduce 12%\n",
      "17/10/17 20:32:24 INFO mapreduce.Job:  map 42% reduce 12%\n",
      "17/10/17 20:32:25 INFO mapreduce.Job:  map 44% reduce 12%\n",
      "17/10/17 20:32:26 INFO mapreduce.Job:  map 47% reduce 15%\n",
      "17/10/17 20:32:27 INFO mapreduce.Job:  map 50% reduce 15%\n",
      "17/10/17 20:32:28 INFO mapreduce.Job:  map 52% reduce 15%\n",
      "17/10/17 20:32:29 INFO mapreduce.Job:  map 53% reduce 17%\n",
      "17/10/17 20:32:30 INFO mapreduce.Job:  map 55% reduce 17%\n",
      "17/10/17 20:32:31 INFO mapreduce.Job:  map 57% reduce 17%\n",
      "17/10/17 20:32:32 INFO mapreduce.Job:  map 58% reduce 19%\n",
      "17/10/17 20:32:33 INFO mapreduce.Job:  map 60% reduce 19%\n",
      "17/10/17 20:32:34 INFO mapreduce.Job:  map 62% reduce 19%\n",
      "17/10/17 20:32:35 INFO mapreduce.Job:  map 64% reduce 21%\n",
      "17/10/17 20:32:36 INFO mapreduce.Job:  map 66% reduce 21%\n",
      "17/10/17 20:32:37 INFO mapreduce.Job:  map 68% reduce 21%\n",
      "17/10/17 20:32:38 INFO mapreduce.Job:  map 70% reduce 23%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/10/17 20:32:39 INFO mapreduce.Job:  map 72% reduce 23%\n",
      "17/10/17 20:32:40 INFO mapreduce.Job:  map 74% reduce 23%\n",
      "17/10/17 20:32:41 INFO mapreduce.Job:  map 76% reduce 25%\n",
      "17/10/17 20:32:42 INFO mapreduce.Job:  map 79% reduce 25%\n",
      "17/10/17 20:32:43 INFO mapreduce.Job:  map 80% reduce 25%\n",
      "17/10/17 20:32:44 INFO mapreduce.Job:  map 81% reduce 26%\n",
      "17/10/17 20:32:45 INFO mapreduce.Job:  map 82% reduce 26%\n",
      "17/10/17 20:32:46 INFO mapreduce.Job:  map 83% reduce 26%\n",
      "17/10/17 20:32:47 INFO mapreduce.Job:  map 83% reduce 28%\n",
      "17/10/17 20:32:49 INFO mapreduce.Job:  map 84% reduce 28%\n",
      "17/10/17 20:32:51 INFO mapreduce.Job:  map 85% reduce 28%\n",
      "17/10/17 20:32:55 INFO mapreduce.Job:  map 86% reduce 28%\n",
      "17/10/17 20:32:57 INFO mapreduce.Job:  map 86% reduce 29%\n",
      "17/10/17 20:33:00 INFO mapreduce.Job:  map 87% reduce 29%\n",
      "17/10/17 20:33:15 INFO mapreduce.Job:  map 88% reduce 29%\n",
      "17/10/17 20:33:40 INFO mapreduce.Job:  map 89% reduce 29%\n",
      "17/10/17 20:33:43 INFO mapreduce.Job:  map 89% reduce 30%\n",
      "17/10/17 20:34:06 INFO mapreduce.Job:  map 90% reduce 30%\n",
      "17/10/17 20:34:33 INFO mapreduce.Job:  map 91% reduce 30%\n",
      "17/10/17 20:34:57 INFO mapreduce.Job:  map 92% reduce 30%\n",
      "17/10/17 20:34:58 INFO mapreduce.Job:  map 92% reduce 31%\n",
      "17/10/17 20:35:20 INFO mapreduce.Job:  map 93% reduce 31%\n",
      "17/10/17 20:35:47 INFO mapreduce.Job:  map 94% reduce 31%\n",
      "17/10/17 20:36:12 INFO mapreduce.Job:  map 95% reduce 31%\n",
      "17/10/17 20:36:14 INFO mapreduce.Job:  map 95% reduce 32%\n",
      "17/10/17 20:36:36 INFO mapreduce.Job:  map 96% reduce 32%\n",
      "17/10/17 20:37:04 INFO mapreduce.Job:  map 97% reduce 32%\n",
      "17/10/17 20:37:25 INFO mapreduce.Job:  map 98% reduce 32%\n",
      "17/10/17 20:37:26 INFO mapreduce.Job:  map 98% reduce 33%\n",
      "17/10/17 20:37:30 INFO mapreduce.Job:  map 99% reduce 33%\n",
      "17/10/17 20:37:31 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "17/10/17 20:37:40 INFO mapreduce.Job:  map 100% reduce 34%\n",
      "17/10/17 20:37:43 INFO mapreduce.Job:  map 100% reduce 48%\n",
      "17/10/17 20:37:46 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "17/10/17 20:37:58 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "17/10/17 20:38:01 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "17/10/17 20:38:04 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "17/10/17 20:38:07 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "17/10/17 20:38:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/10/17 20:38:08 INFO mapreduce.Job: Job job_1505269880969_0482 completed successfully\n",
      "17/10/17 20:38:08 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=290504884\n",
      "\t\tFILE: Number of bytes written=833905342\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=178524573791\n",
      "\t\tHDFS: Number of bytes written=14437511\n",
      "\t\tHDFS: Number of read operations=4653\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=1550\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1305\n",
      "\t\tRack-local map tasks=245\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=49923612\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1076049\n",
      "\t\tTotal time spent by all map tasks (ms)=16641204\n",
      "\t\tTotal time spent by all reduce tasks (ms)=358683\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16641204\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=358683\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=214538401968\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4624141236\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1232799308\n",
      "\t\tMap output records=13932260\n",
      "\t\tMap output bytes=262640358\n",
      "\t\tMap output materialized bytes=290514178\n",
      "\t\tInput split bytes=168950\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=654677\n",
      "\t\tReduce shuffle bytes=290514178\n",
      "\t\tReduce input records=13932260\n",
      "\t\tReduce output records=1309350\n",
      "\t\tSpilled Records=27864520\n",
      "\t\tShuffled Maps =1550\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1550\n",
      "\t\tGC time elapsed (ms)=375280\n",
      "\t\tCPU time spent (ms)=14814860\n",
      "\t\tPhysical memory (bytes) snapshot=4011973787648\n",
      "\t\tVirtual memory (bytes) snapshot=20611538493440\n",
      "\t\tTotal committed heap usage (bytes)=4268902318080\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=178524404841\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=14437511\n",
      "17/10/17 20:38:08 INFO streaming.StreamJob: Output directory: task_usage/output-top-20-cpu\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R task_usage/output-top-20-cpu\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input task_usage/part-*-of-00500.csv \\\n",
    "    -output task_usage/output-top-20-cpu \\\n",
    "    -file ./cpuUsageMapper.py \\\n",
    "    -mapper cpuUsageMapper.py \\\n",
    "    -file ./cpuUsageReducer.py \\\n",
    "    -reducer cpuUsageReducer.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 CPU Usage\r\n",
      "[ID]\t[CPU_Usage]\r\n",
      "6283917433\t46093.074042\r\n",
      "\r\n",
      "6279559429\t29227.922307\r\n",
      "\r\n",
      "6436963420\t28976.109380\r\n",
      "\r\n",
      "6477911130\t26732.013299\r\n",
      "\r\n",
      "6440940956\t22211.296463\r\n",
      "\r\n",
      "6309295531\t20152.227544\r\n",
      "\r\n",
      "6415219880\t18656.967729\r\n",
      "\r\n",
      "6409272482\t16615.456043\r\n",
      "\r\n",
      "6301702571\t15890.345033\r\n",
      "\r\n",
      "6330480082\t15227.012024\r\n",
      "\r\n",
      "6304228795\t14203.806631\r\n",
      "\r\n",
      "6427605842\t12768.415754\r\n",
      "\r\n",
      "6455601542\t12728.731480\r\n",
      "\r\n",
      "6316799672\t11650.149211\r\n",
      "\r\n",
      "6419322512\t11107.614565\r\n",
      "\r\n",
      "6464342149\t10489.754953\r\n",
      "\r\n",
      "6282808418\t10255.289219\r\n",
      "\r\n",
      "6290003708\t10219.845489\r\n",
      "\r\n",
      "6288046928\t8661.764749\r\n",
      "\r\n",
      "6305645816\t8023.026780\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat ./task_usage/output-top-20-cpu/part-00000 | python cpuUsageSorter.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
